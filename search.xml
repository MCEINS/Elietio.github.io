<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Oracle批量插入数据异常 java.lang.ArrayIndexOutOfBoundsException]]></title>
    <url>%2F2019%2F08%2FOracle%20java.lang.ArrayIndexOutOfBoundsException.html</url>
    <content type="text"><![CDATA[今天测试过程中一个Oracle mybatis批量插入数据的代码报出了一个异常 Caused by: java.lang.ArrayIndexOutOfBoundsException: -32768具体异常堆栈信息如下：1234567891011121314Caused by: java.lang.ArrayIndexOutOfBoundsException: -32768 at oracle.jdbc.driver.OraclePreparedStatement.setupBindBuffers(OraclePreparedStatement.java:2673) at oracle.jdbc.driver.OraclePreparedStatement.processCompletedBindRow(OraclePreparedStatement.java:2206) at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3365) at oracle.jdbc.driver.OraclePreparedStatement.execute(OraclePreparedStatement.java:3476) at com.alibaba.druid.filter.FilterChainImpl.preparedStatement_execute(FilterChainImpl.java:3409) at com.alibaba.druid.wall.WallFilter.preparedStatement_execute(WallFilter.java:619) at com.alibaba.druid.filter.FilterChainImpl.preparedStatement_execute(FilterChainImpl.java:3407) at com.alibaba.druid.filter.FilterAdapter.preparedStatement_execute(FilterAdapter.java:1080) at com.alibaba.druid.filter.FilterChainImpl.preparedStatement_execute(FilterChainImpl.java:3407) at com.alibaba.druid.filter.FilterEventAdapter.preparedStatement_execute(FilterEventAdapter.java:440) at com.alibaba.druid.filter.FilterChainImpl.preparedStatement_execute(FilterChainImpl.java:3407) at com.alibaba.druid.proxy.jdbc.PreparedStatementProxyImpl.execute(PreparedStatementProxyImpl.java:167) at com.alibaba.druid.pool.DruidPooledPreparedStatement.execute(DruidPooledPreparedStatement.java:498) 感觉很奇怪，查看日志SQL打印，这个方也就拼接了400条SQL，参数也不是很多，于是从日志里面copy了一下接口入参在本地用Postman debug了一下，结果第一次居然入库成功，再执行一次，出现了同样的错误，再执行，又成功……反复如此，顿时懵逼。看异常信息，执行ojdbc包内的oracle.jdbc.driver.OraclePreparedStatement.setupBindBuffers方法数组下标越界，好吧，放Google搜一下，发现一段这样描述 The 10g driver apparently keeps a global serialnumber for all parameters in the entire batch, with a “short”variable. So you can have at most 32768 parameters in the batch. I was havingthe same exception because I have a INSERT statement with 42 parameters and mybatches can be as big as 1000 records, so 42000 &gt; 32768 and this overflowsto a negative index. I reduced the batch factor to 100 to be safe, and all iswell. I guess your update DML should have a larger number of parameters perrecord, right? (My diagnostic of the bug is just deduction from the symptoms)https://community.oracle.com/thread/599441?start=15&amp;tstart=0&gt; 说是10g driver statement最大允许参数个数为32768，超过会报错。似乎有点类似，但是我只插入了400条啊，而且每个SQL参数只有9个，也就是3600个参数，远小于32768。还有另外一个说法 In Oracle Metalink (Oracle’s support site - Note ID 736273.1) I found that this is a bug in JDBC adapter (version 10.2.0.0.0 to 11.1.0.7.0) that when you call preparedStatement with more than 7 positional parameters then JDBC will throw this error.https://stackoverflow.com/questions/277744/jdbc-oracle-arrayindexoutofboundsexception 感觉也不符合，但是从搜索的结果看，10g 的 ojdbc似乎确实有些问题，于是看了下pom，乖乖12345&lt;dependency&gt; &lt;groupId&gt;ojdbc14&lt;/groupId&gt; &lt;artifactId&gt;ojdbc14&lt;/artifactId&gt; &lt;version&gt;10.2.0.4.0&lt;/version&gt;&lt;/dependency&gt; 那么换个版本吧，我们数据库是11.2g的，于是换了个ojdbc612345&lt;dependency&gt; &lt;groupId&gt;com.oracle&lt;/groupId&gt; &lt;artifactId&gt;ojdbc6&lt;/artifactId&gt; &lt;version&gt;11.2.0.4.0&lt;/version&gt;&lt;/dependency&gt; 嗯，这次很顺利，没有再出现异常，看来ojdbc14确实有些问题，但是还是比较疑惑，单单400条数据，每条9个参数就已经超过限制了吗？1234567891011121314&lt;insert id="insertBondMarkerAlBatch" parameterType="java.util.List"&gt; INSERT INTO CM_PB_BOND_TENDER_HISTORY (HSTRY_ID, PB_BOND_ID, APPID, SR_NO_ID, GRP_ID, TNDR_PRC, THE_REF_YLD, CRT_TM, UPD_TM ) SELECT CM_PB_BOND_TENDER_HISTORY_SEQ.nextval,A.* FROM ( &lt;foreach collection="list" item="item" index="index" separator="union all"&gt; SELECT #&#123;item.pbBondId,jdbcType=BIGINT&#125;, #&#123;item.appId,jdbcType=VARCHAR&#125;, #&#123;item.srNoId,jdbcType=BIGINT&#125;, #&#123;item.grpId,jdbcType=VARCHAR&#125;, #&#123;item.tndrPrc,jdbcType=VARCHAR&#125;, #&#123;item.theRefYld,jdbcType=VARCHAR&#125;, #&#123;item.crtTm,jdbcType=TIMESTAMP&#125;, #&#123;item.updTm,jdbcType=TIMESTAMP&#125; FROM dual &lt;/foreach&gt;) A &lt;/insert&gt; 拼接下来实际SQL如下，类似于insert into tableA select * from tableB12345678910111213INSERT INTO CM_PB_BOND_TENDER_HISTORY (HSTRY_ID, PB_BOND_ID, APPID, SR_NO_ID, GRP_ID, TNDR_PRC, THE_REF_YLD, CRT_TM, UPD_TM) SELECT CM_PB_BOND_TENDER_HISTORY_SEQ.nextval, A.* FROM ( SELECT ?, ?, ?, ?, ?, ?, ?, ? FROM dual union all SELECT ?, ?, ?, ?, ?, ?, ?, ? FROM dual union all SELECT ?, ?, ?, ?, ?, ?, ?, ? FROM dual union all ...... ) A 实际debug了一下，确实出异常的时候OraclePreparedStatement.setupBindBuffers方法short数组 bindIndicators大小超过了32768，换成ojdbc6的时候该方法未调用。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Oracle</tag>
        <tag>异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[惨痛，笔记本机械硬盘突然跪了]]></title>
    <url>%2F2019%2F08%2FHDD.html</url>
    <content type="text"><![CDATA[感觉可能昨天都跪了，一直没注意，今天晚上才发现只剩一个固态C盘了，设备管理器也找不到机械硬盘，拔了重插也不转，心塞，明明这块1T日立也就2年多啊，通电也就6000h，我对日立还特有好感的说，买了好几块了，想到好多东西也没备份就蛋疼 从各个论坛收集的歌曲、图包、漫画，还有各种软件、文档、配置等等，虽然一部分网盘有备份，但是全部拖下来也得费不少功夫，而且有些东西完全想不起了。。。唯一庆幸的是博客的之前备份了一份到Git上，hexo和主题多设备同步，o(︶︿︶)o 唉，以后还是多做备份吧。 新买的2T西数蓝盘，空空如也。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>硬盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jedis Unexpected end of stream 异常]]></title>
    <url>%2F2019%2F05%2FJedis-Unexpected-end-of-stream.html</url>
    <content type="text"><![CDATA[这周末项目生产要上一个修数版本，其中一步是从Mysql中的临时表查找数据然后拼装key从Redis中查找对应的缓存数据并修改。然而升级过程中修数程序却抛出一个异常Unexpected end of stream意外停止。 12345redis.clients.jedis.exceptions.JedisConnectionException: Unexpected end of stream. at redis.clients.util.RedisInputStream.ensureFill(RedisInputStream.java:199) at redis.clients.util.RedisInputStream.readByte(RedisInputStream.java:40) at redis.clients.jedis.Protocol.process(Protocol.java:151)...... 需要修数的Redis采用的是codis搭建的集群，我们立即Telnet访问的端口，并在服务器使用Redis-cli直接连接，结果都未发现异常。于是立即去网上查找相关资料，网上都这样描述此异常。 客户端缓冲区异常这个异常是客户端缓冲区异常，产生这个问题可能有三个原因： 多个线程使用一个Jedis连接。 客户端缓冲区满了,Redis有三种客户端缓冲区： 普通客户端缓冲区(normal)：用于接受普通的命令，例如get、set、mset、hgetall、zrange等。 slave客户端缓冲区(slave)：用于同步master节点的写命令，完成复制。 发布订阅缓冲区(pubsub)：pubsub不是普通的命令，因此有单独的缓冲区。 Redis客户端缓冲区配置的格式是： 1client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt; class: 客户端类型：可选值为normal、slave和pubsub。 hard limit: 如果客户端使用的输出缓冲区大于hard limit，客户端会被立即关闭，单位为秒。 soft limit和soft seconds: 如果客户端使用的输出缓冲区超过了soft limit并且持续了soft limit秒，客户端会被立即关闭，单位为秒。 长时间闲置连接会被服务端主动断开，可以查询timeout配置的设置以及自身连接池配置确定是否需要做空闲检测。 于是我们立即对配置进行了检查，并未发现有相关问题，timeout默认设置为0，客户端缓冲区临时修改为不限制也未见生效。 这下犯愁了，由于这个修数程序几乎没有日志，代码也不是我们编写，而是一位外地的同事提供的。只能一波人紧急分析代码，一波人继续查询错误日志，从异常的堆栈中我们发现异常是从执行redis的一个get方法抛出的，难道是get某个key的时候出现了异常？由于日志中没有打印具体的key信息，所以也不清楚具体情况，难道是某个key的体积过大，导致查询的时候超过了限制？于是立即使用bigkeys查询了一下Redis的大体积key，结果最大的也只有十几kb，很显然这也不是原因。大家正在一筹莫展，准备把程序加上详细的日志再具体分析，但是生产环境做紧急变更交付物是很困难的，之前我们已经在模拟环境同步生产数据测了好几轮，都未发现问题，为何生产就出现问题了呢？ 继续看codis porxy的日志，发现了一个特殊的地方，客户端建立的连接每次都是经过60s后被断开，显示EOF错误，代表客户端客户端主动断开，于是我们立即查找相关配置，是否存在60s的配置，这时候运维同学提到了一件事，codis-porxy使用了nginx做负载均衡代理，nginx应该做了超时配置，于是我们翻看了nginx配置，果然存在一个60s的超时配置，而这时候我们再去翻看代码逻辑，发现了一个问题，首先程序使用jedis建立一个redis连接，然后从MySQL从查找临时表所有数据，然后修改redis缓存。而我们的临时表数据过于庞大，而且缺乏索引，所以查询这一步花费了很长时间，已经超过60s，等数据查询完毕，再执行redis操作，此时一直空闲的连接已经被nginx当作超时给断开了。为何之前模拟环境并未出现这个问题？应该是最近待修的数据又增加了很多，刚好超过60s，导致这个问题并未在模拟阶段发现。 于是我们在模拟环境复现该问题，并临时把nginx配置修改为600s，于是修数程序正常执行，没有异常，问题到此解决，于是生产同步操作，最终完成了升级。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nintendo Switch更换内存卡]]></title>
    <url>%2F2019%2F04%2Freplace-nintendoswitch-sd.html</url>
    <content type="text"><![CDATA[NS入手一年多了，一直拿张32G的microSD卡凑合着，因为入手游戏不多，又都是实体版，所以下载几个DLC也完全够了，不过考虑到实体版换卡还是麻烦，而且今年感兴趣的游戏也不少，有些不打算买实体了，所以一直想找个机会换张大一点的microSD卡，正好狗东200G闪迪卡活动195，价格还算OK，于是入手换之。 准备： Nintendo Switch 闪迪（SanDisk）200GB TF（MicroSD）存储卡 U1 C10 A1 高速读卡器一枚 PC 换卡流程：1. 取卡：NS 关机（Power Off），取出原来的 MicroSD 卡，并把MicroSD卡内容备份到PC2. NS插入新卡（可能需要更新系统）3. 格式化新卡（建议）：设置 - 系统设置 - 格式化选项 - 格式化 microSD 卡，格式化新卡。4. 拷贝游戏文件：将新卡里的 Nintendo 文件夹删除，同时将旧卡备份到PC里的 Nintendo 文件夹复制到新卡里5. 插入新卡：在关机的状态下插入新卡，然后开机，检查下载内容是否存在。]]></content>
      <categories>
        <category>电子产品</category>
      </categories>
      <tags>
        <tag>电子产品</tag>
        <tag>Nintendo Switch</tag>
        <tag>NS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP的三次握手和四次挥手]]></title>
    <url>%2F2019%2F04%2Ftcp-handshake-wave.html</url>
    <content type="text"><![CDATA[TCP（Transmission Control Protoco），是一种基于字节流面向连接的传输层协议。数据的传输需要通信双方建立一个连接，TCP协议采用三次握手建立一个连接，采用 4 次挥手来关闭一个连接。每一个TCP连接都有两个端点，叫作套接字（socket），它的定义为IP地址+端口号拼接。 TCP/IP协议概况 IPv4: 网际协议版本4(Internet Protocol version 4), 使用32位地址 IPv6: 网际协议版本6(Internet Protocol version 6), 使用128位地址，是IPv4替代品，通常把它两者称为”IP“ TCP: 传输控制协议(Transmission Control Protocol),TCP是一个面向连接的协议,为用户进程提供可靠的全双工字节流， TCP套接字是一种流套接字(stream sockte), 关心确认, 超时, 重传等细节 UDP: 用户数据报协议(User Datagram Protocol), UDP是一个无连接协议,UDP套接字是一种数据报套接字(datagram socket) SCTP：流控制传输协议(Stream Control Transmission Protocol),SCTP是一个提供可靠全双工关联的面向连接的协议 ICMP：网际控制消息协议(Internet Control Message Protocol),处理在路由器和主机之间流通的错误和控制消息 ICMPv6：网际控制消息协议版本6 IGMP: 网际组管理协议,用于多播 ARP: 地址解析协议(Address Resolution Protocol),把IPv4地址映射成一个硬件地址(如以太网地址) RARP: 反地址解析协议(Reverse..), 将硬件地址映射成IPv4地址 BPF: BSD分组过滤器 TCP通信三部曲 建立：三次握手 传输：超时重传、快速重传、流量控制、拥塞控制等 断开：四次挥手 TCP服务模型一个 TCP 连接由一个 4 元组构成，分别是两个 IP 地址和两个端口号。一个 TCP 连接通常分为三个阶段：启动、数据传输、关闭。 当 TCP 接收到另一端的数据时，它会发送一个确认，但这个确认不会立即发送，一般会延迟一会儿。ACK 是累积的，一个确认字节号 N 的 ACK 表示所有直到 N 的字节（不包括 N）已经成功被接收了。这样的好处是如果一个 ACK 丢失，很可能后续的 ACK 就足以确认前面的报文段了。 一个完整的 TCP 连接是双向和对称的，数据可以在两个方向上平等地流动。给上层应用程序提供一种双工服务。一旦建立了一个连接，这个连接的一个方向上的每个 TCP 报文段都包含了相反方向上的报文段的一个 ACK。 序列号的作用是使得一个 TCP 接收端可丢弃重复的报文段，记录以杂乱次序到达的报文段。因为 TCP 使用 IP 来传输报文段，而 IP 不提供重复消除或者保证次序正确的功能。另一方面，TCP 是一个字节流协议，绝不会以杂乱的次序给上层程序发送数据。因此 TCP 接收端会被迫先保持大序列号的数据不交给应用程序，直到缺失的小序列号的报文段被填满。 TCP报文头部 源端口和目的端口，各占2个字节； 序号，占4个字节，TCP连接中传送的字节流中的每个字节都按顺序编号； 确认号，占4个字节，是期望收到对方下一个报文的第一个数据字节的序号，即最后被成功接收的数据字节序列号加 1，这个字段只有在 ACK 位被启用的时候才有效； 数据偏移，占4位，它指出TCP报文的数据距离TCP报文段的起始处有多远； 保留，占6位，保留今后使用，但目前应都位0； 紧急URG，当URG=1，表明紧急指针字段有效。告诉系统此报文段中有紧急数据； 确认ACK，仅当ACK=1时，确认号字段才有效。TCP规定，在连接建立后所有报文的传输都必须把ACK置1； 推送PSH，当两个应用进程进行交互式通信时，有时在一端的应用进程希望在键入一个命令后立即就能收到对方的响应，这时候就将PSH=1； 复位RST，当RST=1，表明TCP连接中出现严重差错，必须释放连接，然后再重新建立连接； 同步SYN，在连接建立时用来同步序号。当SYN=1，ACK=0，表明是连接请求报文，若同意连接，则响应报文中应该使SYN=1，ACK=1； 终止FIN，用来释放连接。当FIN=1，表明此报文的发送方的数据已经发送完毕，并且要求释放； 窗口，占2字节，指的是通知接收方，发送本报文你需要有多大的空间来接受； 检验和，占2字节，校验首部和数据这两部分； 紧急指针，占2字节，指出本报文段中的紧急数据的字节数； TCP状态转换 状 态 描 述 CLOSED 关闭状态，没有连接活动或正在进行 LISTEN 监听状态，服务器正在等待连接进入 SYN_RCVD 收到一个连接请求，尚未确认 SYN_SENT 已经发出连接请求，等待确认 ESTABLISHED 连接建立，正常数据传输状态 FIN_WAIT_1 （主动关闭）已经发送关闭请求，等待确认 FIN_WAIT_2 （主动关闭）收到对方关闭确认，等待对方关闭请求 TIMED_WAIT 完成双向关闭，等待所有分组死掉 CLOSING 双方同时尝试关闭，等待对方确认 CLOSE_WAIT （被动关闭）收到对方关闭请求，已经确认 LAST_ACK （被动关闭）等待最后一个关闭确认，并等待所有分组死掉 那么状态转换为什么要经历三次握手和四次挥手呢？ TCP三次握手 服务器进程先创建传输控制块TCB，时刻准备接受客户进程的连接请求，此时服务器就进入了LISTEN（监听）状态； 客户端先创建传输控制块TCB，然后向服务器发出连接请求报文，SYN=1，表示请求建立连接，同时选择一个初始序列号 seq=x ，此时，TCP客户端进程进入了 SYN_SENT（同步已发送状态）状态。TCP规定，SYN报文段（SYN=1的报文段）不能携带数据，但需要消耗掉一个序号。 服务器收到请求报文后，如果同意连接，则发出确认报文。确认报文中应该 ACK=1，SYN=1，确认号是ack=x+1，同时也要为自己初始化一个序列号 seq=y，此时，TCP服务器进程进入了SYN_RCVD（同步收到）状态。这个报文也不能携带数据，但是同样要消耗一个序号。 T客户端收到确认后，还要向服务器给出确认。确认报文的ACK=1，ack=y+1，自己的序列号seq=x+1，此时，TCP连接建立，客户端进入ESTABLISHED（已建立连接）状态。TCP规定，ACK报文段可以携带数据，但是如果不携带数据则不消耗序号。 当服务器收到客户端的确认后也进入ESTABLISHED状态，三次握手结束，连接建立。 为什么需要第三次客户端ACK？ 三次握手的目的是“为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误。例如网络出现抖动导致第一次SYN=1并没有及时到达服务端，从而客户端无法收到服务端的ACK，会触发客户端的重试。如果只有两次握手，第二次重发建立连接后，第一次延迟的请求刚好到达服务端，服务端会再次回包ACK，这样本来无效的请求又会建立新的连接。如果是三次握手，客户端收到服务端ACK并不会再次发出确认，这样就不会有新的连接建立。 TCP四次挥手以客户端主动关闭为例，服务器端也可以主动关闭，方向与下面相反。 客户端进程发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN=1，其序列号为seq=u（等于前面已经传送过来的数据的最后一个字节的序号加1），此时，客户端进入FIN_WAIT_1（终止等待1）状态。 TCP规定，FIN报文段即使不携带数据，也要消耗一个序号。 服务器收到连接释放报文，发出确认报文，ACK=1，ack=u+1，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE_WAIT（关闭等待）状态。TCP服务器通知高层的应用进程，客户端向服务器的方向就释放了，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是服务器若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE_WAIT状态持续的时间。 客户端收到服务器的确认请求后，此时，客户端就进入FIN_WAIT_2（终止等待2）状态，等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）。 服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST_ACK（最后确认）状态，等待客户端的确认。 客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME_WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2*MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。 服务器只要收到了客户端发出的确认，立即进入CLOSED状态，撤销TCB结束TCP连接。 为什么主动关闭连接方最后需要等待2MSL？ MSL（Maximum Segment Lifetime），报文最大生存时间。保证客户端发送的最后一个ACK报文能够到达服务器，如果因为网络原因导致服务器没收到，服务器会重新发送一次FIN+ack请求关闭连接，客户端就能收到这个重传的报文再次ACK，并且重启2MSL计时器。防止类似与“三次握手”中提到了的“已经失效的连接请求报文段突然又传送到了服务端，因而产生错误”。客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，新的连接中不会出现旧连接的请求报文。需要注意的是在TIME_WAIT状态 时两端的端口不能使用，客户端要等到2MSL时间结束才可继续使用。 为什么关闭连接需要四次挥手，比建立连接多一次呢？ 建立连接的时候， 服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，ACK和SYN是分开发送的，服务器收到对方的FIN报文时，仅仅表示客户端请求关闭但是还能接收数据，而且服务器的数据可能也没有发送完毕，所以服务器可以选择立即关闭FIN也可以再发送一部分数据然后再发送FIN报文给客户端关闭连接，由于服务器ACK和FIN分开发送，所以多了一次。 如果通信双方同时请求连接或同时请求释放连接？ 这种情况虽然发生的可能性极小，但是是确实存在的，TCP也特意设计了相关机制，使得在这种情况下双方仅建立一条连接。双方同时请求连接的情况下，双方同时发出请求连接报文，并进入SYN_SENT状态；当收到对方的请求连接报文后，会再次发送请求连接报文，确认号为对方的SYN+1，并进入SYN_RCVD状态；当收到对方第二次发出的携带确认号的请求报文之后，会进入ESTABLISHED状态。 双方同时请求释放连接也是同样的，双方同时发出连接释放报文，并进入FIN_WAIT_1状态；在收到对方的报文之后，发送确认报文，并进入CLOSING状态；在收到对方的确认报文后，进入TIME_WAIT状态，等待2MSL之后关闭连接。需要注意的是，这个时候虽然不用再次发送确认报文并确认对方收到，双方仍需等待2MSL之后再关闭连接，是为了防止“已失效的连接请求报文段”的影响。 TIME_WAIT状态 首先TIME_WAIT状态是执行主动关闭的那一端产生的，从上面2MSL中我们了解TIME_WAIT状态有两个存在的理由:可靠地实现TCP全双工连接的终止，即最后一次ACK如果丢失，可以重新发送FIN并再次ACK；允许老的重复分节在网络中消逝，新的连接中不会出现旧连接的请求报文；在高并发短连接的TCP服务器上，当服务器处理完请求后立刻按照主动正常关闭连接。这个场景下，会出现大量socket处于TIME_WAIT状态。如果客户端的并发量持续很高，大量端口处于TIME_WAIT状态，无法正常使用，此时部分客户端就会显示连接不上，所以需要引起重视。 参考📚：TCP的三次握手与四次挥手]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自建Shadowsocks频繁被封？赶紧试试simple-obfs流量混淆吧]]></title>
    <url>%2F2019%2F04%2Fshadowsocks-simple-obfs.html</url>
    <content type="text"><![CDATA[年后用的一直挺稳的搬瓦工VPS搭建的Shadowsocks端口频繁被封，换了几次端口后感觉太麻烦，于是决定网上找找有没有好的思路，于是发现了simple-obfs,其思路是ss客户端对Shadowsocks流量混淆加密伪装成正常http或者https流量，服务端ss-server再对流量解密，以此躲过GFW的封杀。这个方案挺有意思，很多人又再此基础上进行了更多的伪装扩展，稳定性比之前纯Shadowsocks流加密更安全。不过原作者已经不再维护此项目而是转入了v2ray-plugin，由于本人使用的路由器SS固件只支持simple-obfs插件模式，因此决定还是先试试simple-obfs的效果。需要： shadowsocks-libev simple-obfs VPS Nginx 支持simple-obfs插件模式的ss客户端 安装首先安装shadowsock，使用shadowsocks-libev,此项目支持插件模式使用simple-obfs，具体部署可以查看github主页。当然如果图省事也可以使用别人的一键安装脚本，例如shadowsocks_install, 123wget --no-check-certificate -O shadowsocks-all.sh &lt;https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh&gt;chmod +x shadowsocks-all.sh./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log 输入具体参数一路向下即可。 配置ss-server：最后生成的/etc/shadowsocks-libev/config.json配置如下格式。 12345678910111213&#123; "server":"0.0.0.0", "server_port":port, "password":"password", "timeout":300, "user":"nobody", "method":"chacha20-ietf-poly1305", "fast_open":false, "nameserver":"8.8.8.8", "mode":"tcp_and_udp", "plugin":"obfs-server", "plugin_opts":"obfs=http"&#125; method:加密方式，推荐使用AEAD算法，比如chacha20，aes-256-cfb 和 rc4-md5已经不再安全,很容易识别。 plugin：使用obfs-server插件 plugin_opts：混淆参数，obfs 有 tls 和 http 两种类型，根据你混淆的类型设置吧，和后面要保持一致 Nginx：之前我们没启用 simple-obfs，ss-server 和ss-local直接进行数据交换，很容易别识别出来。现在ss-local通过simple-obfs伪装，ss-server 再拿掉这层伪装进行数据交换，这样可靠性就得到提升，我们的目的是让ss-local的流量更像正常的http流量，我们可以在VPS上80端口和443端口部署一个静态站点，并申请一个域名，再用Nginx再做一层反向代理，正常流量直接访问静态站点，而ss-local进行simple-obfs混淆域名的流量则代理到ss-server上去，这样从外表上看我们是去访问VPS上部署的网站，而实际却是和ss-local进行数据交互。 nginx配置参考如下，端口用80、443、8080等常用端口更符合http逻辑。 1234567891011121314151617181920212223server&#123; listen 80; server_name your domain; index index.html index.htm index.php default.html default.htm default.php; charset utf-8; location / &#123; root /home/wwwroot/...; autoindex on; &#125; location =/ &#123; root /home/wwwroot/...; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection "upgrade"; if ($http_upgrade = "websocket") &#123; proxy_pass http://127.0.0.1:ss-local port; &#125; &#125; access_log off;&#125; 好了，服务端的部署到此差不多结束了，下面是客户端的一些配置 Windows:shadowsocks-windows需要安装obfs-local插件，并引入配置中 1234567891011&#123; "server": "your domain", "server_port": 80, "password": "your password", "method": "chacha20-ietf-poly1305", "plugin": "obfs-local", "plugin_opts": "obfs=http;obfs-host=your domain", "plugin_args": "", "remarks": "", "timeout": 5&#125; Android：GitHub | Google Play 下载混淆插件，Shadowsocks Android 客户端配置文件下选取 simple-obfs 插件，配置项： 12Obfuscation wrappper: httpObfuscation hostname: your domain Padavan:如果你的路由器和我一样使用Padavan，可以在ss配置页面设置如下 插件名称：obfs-local 插件参数：obfs=http;obfs-host=your domain 好了，到此整个从客户端到服务端的流量混淆加密完成，开始体验新的模式的吧，个人感觉还是挺稳的。 shadowsocks-libev相关命令12345启动：/etc/init.d/shadowsocks-libev start停止：/etc/init.d/shadowsocks-libev stop重启：/etc/init.d/shadowsocks-libev restart状态：/etc/init.d/shadowsocks-libev status systemctl status shadowsocks-libev 开启BBR加速 BBR 是Google TCP拥塞控制算法，能最大化利用网络上瓶颈链路的带宽，降低网络链路上的 buffer 占用率，从而降低延迟，实际测试对网速提升很大。从 4.9 开始，Linux 内核已经用上了该算法。 安装：推荐使用一键脚本 1wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh &amp;&amp; chmod +x bbr.sh &amp;&amp; ./bbr.sh 安装完成后，脚本会提示需要重启 VPS，输入 y 并回车后重启。 重启完成后，进入 VPS，验证一下是否成功安装最新内核并开启 TCP BBR，输入以下命令： 12$ uname -r4.9.0-3-amd64 查看内核版本，显示为最新版就表示 OK 了 一些检测命令:12$ sysctl net.ipv4.tcp_available_congestion_controlnet.ipv4.tcp_available_congestion_control = bbr cubic reno 12$ sysctl net.ipv4.tcp_congestion_controlnet.ipv4.tcp_congestion_control = bbr 12$ sysctl net.core.default_qdiscnet.core.default_qdisc = fq 12$ lsmod | grep bbrtcp_bbr 16384 28]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>科学上网</tag>
        <tag>Shadowsocks</tag>
        <tag>simple-obfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo和主题多设备同步]]></title>
    <url>%2F2019%2F04%2Fhexo-themes-subtree.html</url>
    <content type="text"><![CDATA[使用hexo可以生成静态网页部署到GitHub和VPS上搭建个人博客，但是hexo的部署都是在本地，如果换了一套环境如何也能够编辑发布自己的博客网站呢？ 同步方案由于部署博客已经使用了github仓库托管网页代码，我们可以考虑使用这个来做hexo部署发布管理的版本控制，由于部署的网站默认使用了master分支，因此我们可以使用一个新的分支hexo或者新建一个仓库来管理。 下面步骤默认已经安装好了hexo并且已经成功部署网站，首先切换到hexo主目录，git init进行初始化，如果已经纳入git管理并且关联了远程仓库，可能需要删除重新关联。 1234567891011121314# git 初始化$ git init# 查看关联的远程仓库$ git remote# 删除$ git remote rm origin# 设置新的远程仓库$ git remote add origin git@github.com:Elietio/Elietio.github.io.git# master分支会作为网站部署分支，因此我们切换一个新的分支hexo$ git checkout -b hexo# 推送远程$ git add .$ git commit -m ""$ git push -u origin hexo 到这一步似乎已经大功告成了，然而，如果你使用了第三方主题，并且是直接git clone，你会发现一个问题，上传的themes/下面主题是空目录，因为git无法直接管理这样的嵌套模块，那么该怎么做呢？最暴力，直接取消主题模块的git管理，但是这样后续主题模块的更新就是一个问题了，所以不推荐，好在我们可以通过git的submodule或subtree来实现，对于git clone安装的其它主题和插件都可以按照这个思路。 git submodule vs git subtree简单来说，submodule 和 subtree 最大的区别是，submodule 保存的是子仓库的 link，而 subtree 保存的是子仓库的 copy。 git submodulechild 目录被当做一个独立的 Git 仓库，所有的 Git 命令都可以在 child 目录以及上层项目下独立工作。尽管 child 是子目录，当你不在 child 目录时并不记录它的内容。而当你在那个子目录里修改并提交时，子项目会通知那里的 HEAD 已经发生变更并记录你当前正在工作的那个提交。而此时上层项目会显示 child 目录下的改动，将它记录成来自那个仓库的一个特殊的提交。 若他人要克隆该项目，会发现 child 目录为空。这时需要执行 git submodule init 来初始化你的本地配置文件，以及 git submodule update 拉取数据并切换到合适的提交。而后每次从主项目拉取子模块的变更时，由于主项目只更新了子模块提交的引用而没有更新子模块目录下的代码，必须执行 git submodule update 来更新子模块代码。 git subtree不同于 git submodule，此时的 child 仅仅是含有相关代码的普通目录，而不是一个独立的 Git 仓库。因此当在 child 进行修改时，上层项目会立刻记录其改动，而不是像之前那样先在子项目中提交才能进行记录。克隆上层仓库时 child 目录也不再为空。但同时，child 也不能再执行独立的 Git 命令，只有 git subtree 相关的操作。 进行操作前请先备份已有的next主题目录，根据不同情况操作中可能需要删除并且重新clone下来 1234567891011121314151617181920首先在自己的github上fork一份next源码git@github.com:Elietio/hexo-theme-next.git # 为hexo添加远程仓库 # git remote add -f &lt;子仓库名&gt; &lt;子仓库地址&gt;$ git remote add -f next git@github.com:Elietio/hexo-theme-next.git# 添加subtree# git subtree add --prefix=&lt;子目录名&gt; &lt;子仓库名&gt; &lt;分支&gt; squash意思是把subtree的改动合并成一次commit$ git subtree add --prefix=themes/next next master --squash# 更新子项目$ git fetch next master$ git subtree pull --prefix=themes/next next master --squash整个项目的pull、push同样会对子项目起作用而对next子项目进行pull、push操作需要使用subtree# git subtree push --prefix=&lt;子目录名&gt; &lt;远程分支名&gt; 分支$ git subtree push --prefix=themes/next next master # git subtree pull --prefix=&lt;子目录名&gt; &lt;远程分支名&gt; 分支$ git subtree pull --prefix=themes/yilia yilia master --squash 其它设备同步上面操作已经把hexo的源目录同步到Git，因此我们只需要clone，并且安装node.js和hexo环境 1234567$ git clone -b hexo git@github.com:Elietio/Elietio.github.io.git# 安装 hexo$ npm install hexo(npm install hexo-cli -g)# 安装依赖库$ npm install # 安装部署相关配置$ npm install hexo-deployer-git]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>hexo</tag>
        <tag>git</tag>
        <tag>subtree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux的awk命令]]></title>
    <url>%2F2019%2F04%2F2019-04-10-shell-awk.html</url>
    <content type="text"><![CDATA[awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。awk 命令和 sed 命令结构相同，通常情况下，awk 将每个输入行解释为一条记录而每一行中的内容（由空格或者制表符分隔）解释为每一个字段，一个或者多个连续空格或者制表符看做定界符。awk 中 $0 代表整个记录。 linux的awk命令1awk ' /MA/ &#123; print $1 &#125;' list 解释：打印包含 MA 的行中的第一个单词。再举一个具体的例子，比如 1echo 'this is one world\nthat is another world' | awk '&#123;print $1&#125;' 那么输出就是 awk 处理之后的每一行第一个字符也就是 12thisthat 基本格式awk 命令的基本格式 1awk [options] 'script' file options 这个表示一些可选的参数选项，script 表示 awk 的可执行脚本代码（一般被{} 花括号包围），这个是必须的。file 这个表示 awk 需要处理的文件，注意需要是纯文本文件（意味着 awk 能够处理）。 awk 自定义分隔符之前提到的awk 默认的分割符为空格和制表符，awk 会根据这个默认的分隔符将每一行分为若干字段，依次用 $1, $2,$3 来表示，可以使用 -F 参数来指定分隔符 1awk -F ':' '&#123;print $1&#125;' /etc/passwd 解释：使用 -F 来改变分隔符为 : ，比如上面的命令将 /etc/passwd 文件中的每一行用冒号 : 分割成多个字段，然后用 print 将第 1 列字段的内容打印输出 在 awk 中同时指定多个分隔符，比如现在有这样一个文件 some.log 文件内容如下 12345Grape(100g)1980raisins(500g)1990plum(240g)1997apricot(180g)2005nectarine(200g)2008 现在我们想将上面的 some.log 文件中按照 “水果名称（重量）年份” 来进行分割 123456$ awk -F '[()]' '&#123;print $1, $2, $3&#125;' some.logGrape 100g 1980raisins 500g 1990plum 240g 1997apricot 180g 2005nectarine 200g 2008 在 -F 参数中使用一对方括号来指定多个分隔符，awk 处理 some.log 文件时就会使用 “(“ 或者 “)” 来对文件的每一行进行分割。 awk 内置变量的使用awk 除了 $ 和数字表示字段还有一些其他的内置变量： $0 这个表示文本处理时的当前行，$1 表示文本行被分隔后的第 1 个字段列，$2 表示文本行被分割后的第 2 个字段列，$3 表示文本行被分割后的第 3 个字段列，$n 表示文本行被分割后的第 n 个字段列 NR 表示文件中的行号，表示当前是第几行 NF 表示文件中的当前行被分割的列数，可以理解为 MySQL 数据表里面每一条记录有多少个字段，所以 $NF 就表示最后一个字段，$(NF-1) 就表示倒数第二个字段 FS 表示 awk 的输入分隔符，默认分隔符为空格和制表符，可以对其进行自定义设置 OFS 表示 awk 的输出分隔符，默认为空格，也可以对其进行自定义设置 FILENAME 表示当前文件的文件名称，如果同时处理多个文件，它也表示当前文件名称 RS 行分隔符，用于分割行，默认为换行符 ORS 输出记录的分隔符，默认为换行符 比如我们有这么一个文本文件 fruit.txt 内容如下，用它来演示如何使用 awk 命令工具 12345678peach 100 Mar 1997 ChinaLemon 150 Jan 1986 AmericaPear 240 Mar 1990 Janpanavocado 120 Feb 2008 chinaawk '&#123;print $0&#125;' fruit.txt # 表示打印输出文件的每一整行的内容awk '&#123;print $1&#125;' fruit.txt # 表示打印输出文件的每一行的第 1 列内容awk '&#123;print $1, $2&#125;' fruit.txt 文件的每一行的每一列的内容除了可以用 print 命令打印输出以外，还可以对其进行赋值 1awk '&#123;$2 = "***"; print $0&#125;' fruit.txt 上面的例子就是表示通过对 $2 变量进行重新赋值，来隐藏每一行的第 2 列内容，并且用星号 * 来代替其输出 在参数列表中加入一些字符串或者转义字符之类的东东 1awk '&#123;print $1 "\t" $2 "\t" $3&#125;' fruit.txt 像上面这样，你可以在 print 的参数列表中加入一些字符串或者转义字符之类的东东，让输出的内容格式更漂亮，但一定要记住要使用双引号。 awk 内置 NR 变量表示每一行的行号 123456awk '&#123;print NR "\t" $0&#125;' fruit.txt1 peach 100 Mar 1997 China2 Lemon 150 Jan 1986 America3 Pear 240 Mar 1990 Janpan4 avocado 120 Feb 2008 china awk 内置 NF 变量表示每一行的列数 123456awk '&#123;print NF "\t" $0&#125;' fruit.txt5 peach 100 Mar 1997 China5 Lemon 150 Jan 1986 America5 Pear 240 Mar 1990 Janpan5 avocado 120 Feb 2008 china awk 中 $NF 变量的使用 awk &apos;{print $NF}&apos; fruit.txt 上面这个 $NF 就表示每一行的最后一列，因为 NF 表示一行的总列数，在这个文件里表示有 5 列，然后在其前面加上 $ 符号，就变成了 $5 ，表示第 5 列 123456awk '&#123;print $(NF - 1)&#125;' fruit.txt1997198619902008 上面 $(NF-1) 表示倒数第 2 列， $(NF-2) 表示倒数第 3 列，依次类推。 1234567awk 'NR % 6' # 打印出了 6 倍数行之外的其他行awk 'NR &gt; 5' # 打印第 5 行之后内容，类似 `tail -n +6` 或者 `sed '1,5d'`awk 'NF &gt;= 6' # 打印大于等于 6 列的行awk '/foo/ &amp;&amp; /bar/' # 打印匹配 `/foo/` 和 `/bar/` 的行awk '/foo/ &amp;&amp; !/bar/' # 打印包含 `/foo/` 不包含 `/bar/` 的行awk '/foo/ || /bar/' # 或awk '/foo/,/bar/' # 打印从匹配 `/foo/` 开始的行到 `/bar/` 的行，包含这两行 awk 内置函数awk 还提供了一些内置函数，比如 toupper() 用于将字符转为大写 tolower() 将字符转为小写 length() 长度 substr() 子字符串 sin() 正弦 cos() 余弦 sqrt() 平方根 rand() 随机数 更多的方法可以参考 man awk awk 同时处理多个文件1awk '&#123;print FILENAME "\t" $0&#125;' demo1.txt demo2.txt 当你使用 awk 同时处理多个文件的时候，它会将多个文件合并处理，变量FILENAME 就表示当前文本行所在的文件名称。 BEGIN 关键字的使用在脚本代码段前面使用 BEGIN 关键字时，它会在开始读取一个文件之前，运行一次 BEGIN关键字后面的脚本代码段， BEGIN 后面的脚本代码段只会执行一次，执行完之后 awk 程序就会退出 1awk 'BEGIN &#123;print "Start read file"&#125;' /etc/passwd awk 脚本中可以用多个花括号来执行多个脚本代码，就像下面这样 1awk 'BEGIN &#123;print "Start read file"&#125; &#123;print $0&#125;' /etc/passwd END 关键字使用方法awk 的 END 指令和 BEGIN 恰好相反，在 awk 读取并且处理完文件的所有内容行之后，才会执行END后面的脚本代码段 awk &apos;END {print &quot;End file&quot;}&apos; /etc/passwd awk &apos;BEGIN {print &quot;Start read file&quot;} {print $0} END {print &quot;End file&quot;}&apos; /etc/passwd 在 awk 中使用变量可以在 awk 脚本中声明和使用变量 1awk '&#123;msg="hello world"; print msg&#125;' /etc/passwd awk 声明的变量可以在任何多个花括号脚本中使用 1awk 'BEGIN &#123;msg="hello world"&#125; &#123;print msg&#125;' /etc/passwd 在 awk 中使用数学运算，在 awk 中，像其他编程语言一样，它也支持一些基本的数学运算操作 1awk '&#123;a = 12; b = 24; print a + b&#125;' company.txt 上面这段脚本表示，先声明两个变量 a = 12 和 b = 24，然后用 print 打印出 a 加上 b 的结果。 请记住 awk 是针对文件的每一行来执行一次单引号 里面的脚本代码，每读取到一行就会执行一次，文件里面有多少行就会执行多少次，但 BEGIN 和 END 关键字后脚本代码除外，如果被处理的文件中什么都没有，那 awk 就一次都不会执行。 awk 还支持其他的数学运算符 12345+ 加法运算符- 减法运算符* 乘法运算符/ 除法运算符% 取余运算符 在 awk 中使用条件判断 比如有一个文件 company.txt 内容如下 yahoo 100 4500 google 150 7500 apple 180 8000 twitter 120 5000 如果要判断文件的第 3 列数据，也就是平均工资小于 5500 的公司，然后将其打印输出 1awk '$3 &lt; 5500 &#123;print $0&#125;' company.txt 上面的命令结果就是平均工资小于 5500 的公司名单，$3 &lt; 5500 表示当第 3 列字段的内容小于 5500 的时候才会执行后面的 {print $0} 代码块 1awk '$1 == "yahoo" &#123;print $0&#125;' company.txt awk 还有一些其他的条件操作符如下 运算符 描述 &lt; 小于 &lt;= 小于或等于 == 等于 != 不等于 &gt; 大于 &gt;= 大于或等于 ~ 匹配正则表达式 !~ 不匹配正则表达式 使用 if 指令判断来实现上面同样的效果 1awk '&#123;if ($3 &lt; 5500) print $0&#125;' company.txt 上面表示如果第 3 列字段小于 5500 的时候就会执行后面的 print $0 在 awk 中使用正则表达式比如现在我们有这么一个文件 poetry.txt 内容如下： 1234This above all: to thine self be trueThere is nothing either good or bad, but thinking makes it soThere’s a special providence in the fall of a sparrowNo matter how dark long, may eventually in the day arrival 使用正则表达式匹配字符串 “There” ，将包含这个字符串的行打印并输出 1234awk '/There/&#123;print $0&#125;' poetry.txtThere is nothing either good or bad, but thinking makes it soThere’s a special providence in the fall of a sparrow 使用正则表达式配一个包含字母 t 和字母 e ，并且 t 和 e 中间只能有任意单个字符的行 12345awk '/t.e/&#123;print $0&#125;' poetry.txtThere is nothing either good or bad, but thinking makes it soThere’s a special providence in the fall of a sparrowNo matter how dark long, may eventually in the day arrival 如果只想匹配单纯的字符串 “t.e”， 那正则表达式就是这样的 /t.e/ ，用反斜杠来转义 . 符号 因为 . 在正则表达式里面表示任意单个字符。 使用正则表达式来匹配所有以 “The” 字符串开头的行 1awk '/^The/&#123;print $0&#125;' poetry.txt 在正则表达式中 ^表示以某某字符或者字符串开头。 使用正则表达式来匹配所有以 “true” 字符串结尾的行 1awk '/true$/&#123;print $0&#125;' poetry.txt 在正则表达式中 $ 表示以某某字符或者字符串结尾。 123awk '/m[a]t/&#123;print $0&#125;' poetry.txtNo matter how dark long, may eventually in the day arrival 上面这个正则表达式 /m[a]t/ 表示匹配包含字符 m ，然后接着后面包含中间方括号中表示的单个字符 a ，最后包含字符 t 的行，输出结果中只有单词 “matter” 符合这个正则表达式的匹配。因为正则表达式 [a] 方括号中表示匹配里面的任意单个字符。 继续上面的一个新例子如下 1awk '/^Th[ie]/&#123;print $0&#125;' poetry.txt 这个例子中的正则表达式 /^Th[ie]/ 表示匹配以字符串 “Thi” 或者 “The” 开头的行，正则表达式方括号中表示匹配其中的任意单个字符。 再继续上面的新的用法 1awk '/s[a-z]/&#123;print $0&#125;' poetry.txt 正则表达式 /s[a-z]/ 表示匹配包含字符 s 然后后面跟着任意 a 到 z 之间的单个字符的字符串，比如 “se”, “so”, “sp” 等等。 正则表达式 [] 方括号中还有一些其他用法比如下面这些 [a-zA-Z] 表示匹配小写的 a 到 z 之间的单个字符，或者大写的 A 到 Z 之间的单个字符 [^a-z] 符号 `^` 在方括号里面表示取反，也就是非的意思，表示匹配任何非 a 到 z 之间的单个字符 正则表达式中的星号 * 和加号 + 的使用方法，* 表示匹配星号前字符串 0 次或者多次，+ 和星号原理差不多，只是加号表示任意 1 个或者 1 个以上，也就是必须至少要出现一次。 正则表达式问号 ? 的使用方法，正则中的问号 ? 表示它前面的字符只能出现 0 次 或者 1 次。 正则表达式中的 {} 花括号用法，花括号 {} 表示规定它前面的字符必须出现的次数，像这个 /go{2}d/ 就表示只匹配字符串 “good”，也就是中间的字母 “o” 必须要出现 2 次。 正则表达式中的花括号还有一些其他的用法如下 /go{2,10}d/ 表示字母 &quot;o&quot; 只能可以出现 2 次，3 次，4 次，5 次，6 次 ... 一直到 10 次 /go{2,}d/ 表示字母 &quot;o&quot; 必须至少出现 2 次或着 2 次以上 正则表达式中的圆括号表示将多个字符当成一个完整的对象来看待。比如 /th(in){1}king/ 就表示其中字符串 “in” 必须出现 1 次。而如果不加圆括号就变成了 /thin{1}king/ 这个就表示其中字符 “n” 必须出现 1 次。 一些组合使用使用 awk 过滤 history 输出，找到最常用的命令1history | awk '&#123;a[$2]++&#125;END&#123;for(i in a)&#123;print a[i] " " i&#125;&#125;' | sort -rn | head 过滤文件中重复行1awk '!x[$0]++' &lt;file&gt; 将一行长度超过 72 字符的行打印1awk 'length&gt;72' file 查看最近哪些用户使用系统1last | grep -v "^$" | awk '&#123; print $1 &#125;' | sort -nr | uniq -c 假设有一个文本，每一行都是一个 int 数值，想要计算这个文件每一行的和，可以使用1awk '&#123;s+=$1&#125; ENG &#123;printf "%.0f", s&#125;' /path/to/file reference http://www.ruanyifeng.com/blog/2018/11/awk.html]]></content>
      <categories>
        <category>linux</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对于含有泛型的类,如何在类中获取泛型的class对象？]]></title>
    <url>%2F2018%2F05%2Fjava-generics.html</url>
    <content type="text"><![CDATA[对于含有泛型的类,如何在类中获取泛型的class对象？ 如果子类继承该类的时候传递了泛型，所以编译期该类的泛型其实已经指定了。那么在类中定义一个Class对象，然后通过构造代码块，this指向的是当前调用的子类 123456private Class&lt;T&gt; clazz;&#123; // 获取当前类上的泛型(this指向当前子类) ParameterizedType type =(ParameterizedType)this.getClass().getGenericSuperclass(); clazz = (Class&lt;T&gt;) type.getActualTypeArguments()[0]; &#125; 对于没有继承该类的子类，可以采用new的时候有参构造函数传递 123456private Class&lt;T&gt; clazz;//通过构造函数指定元素类型public classname(Class&lt;T&gt; clazz) &#123; super(); this.clazz = clazz; &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap探究以及ConcurrentHashMap]]></title>
    <url>%2F2017%2F11%2Fhashmap-concurrenthashmap.html</url>
    <content type="text"><![CDATA[HashMap是一种结构非常巧妙的集合容器。HashMap是存储键值对的结构，key和value一一对应，key必须保证唯一，存储无序，线程不安全，这是它的一些基本特点。在java中，最基本的结构就是两种，一个是数组，另外一个是模拟指针（引用），所有的数据结构都可以用这两个基本结构来构造的，hashmap也不例外。 Hashmap实际上是一个数组和链表的结合体（在数据结构中，一般称之为“链表散列“），如下图所示 当新建一个HashMap的时候，会初始化一个数组。 123456789//The table, resized as necessary. Length MUST Always be a power of two. //FIXME 这里需要注意这句话，至于原因后面会讲到 transient Entry[ ] table; static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; final int hash; Entry&lt;K,V&gt; next; &#125; Entry&lt;K,V&gt;就是存储在数组中的元素，它包含了一个指向下个元素的引用，构成了一个单向链表。 当我们往hashmap中put元素的时候，先根据key的hash值得到这个元素在数组中的位置（即下标），然后就可以把这个元素放到对应的位置中了。如果这个元素所在的位子上已经存放有其他元素了，根据equals方法进行比较，有相等的则覆盖掉，没有相当的那么将以链表的形式存放，新加入的放在链头，最先加入的放在链尾。从hashmap中get元素时，首先计算key的hashcode，找到数组中对应位置的某一元素，然后通过key的equals方法在对应位置的链表中找到需要的元素。 1.Hash算法我们可以看到在hashmap中要找到某个元素，需要根据key的hash值来求得对应数组中的位置。如何计算这个位置就是hash算法。前面说过hashmap的数据结构是数组和链表的结合，所以如果hashmap里面的元素位置尽量的分布均匀些，尽量使得每个位置上的元素数量只有一个，那么查找的时候无需遍历链表，速度就会很快。Java中是这样实现的 123static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; 首先算得key得hashcode值，然后跟数组的长度-1做一次“与”运算（&amp;）。HashMap默认的初始化数组长度是16，并且无论创建多大的HashMap，它的数组长度都是2的n次方，这是为何呢？ 看下图，左边两组是数组长度为16（2的4次方），右边两组是数组长度为15。两组的hashcode均为8和9，但是很明显，当它们和1110“与”的时候，产生了相同的结果，也就是说它们会定位到数组中的同一个位置上去，这就产生了碰撞，8和9会被放到同一个链表上，那么查询的时候就需要遍历这个链表，得到8或者9，这样就降低了查询的效率。同时，我们也可以发现，当数组长度为15的时候，hashcode的值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！ 因此，当数组的长度为2的n次方的时候，能够尽量保证元素分布均匀。那么我们想要存放1000个元素，new HashMap(1000),java会自动将HashMap的数组长度设为1024，这样到底合适吗？实际上并不合适，HashMap中还有一个装载因子LoadFactor。 HashMap的扩容HashMap含有一个阈值，当达到这个阈值，HashMap就会扩容，重新计算每一个元素的Hash，放到新的位置，默认扩容一倍。扩容也是为了减少元素碰撞的机率，阈值=数组长度xLoadFactor，默认LoadFactor为0.75。因此，回到上面，当我们想要存储1000个元素的时候，如果设置HashMap大小为1024，那么阈值为1024x0.75=768，即超过768就会发生扩容，会将数组长度扩大到2048，然后重新计算每个元素在数组中的位置，这是一个非常消耗资源的操作，因此正确的做法是应该new HashMap(2048)比较合适。我们说道HashMap是不安全的，它的不安全也主要体现在扩容的时候。 HashMap的安全问题在多线程操作时，扩容的时候如果多线程都进行reHash操作，那么有可能造成链表中的元素相互引用，即A的下一个指向B，B的下一个指向A，造成了死循环，从而导致线程不安全。如何让HashMap变成安全的？有三种方法。 替换成Hashtable，Hashtable通过对整个表上锁实现线程安全，因此效率比较低 使用Collections类的synchronizedMap方法包装一下。方法如下： 1public static &lt;K,V&gt; Map&lt;K,V&gt; synchronizedMap(Map&lt;K,V&gt; m) 使用ConcurrentHashMap，它使用分段锁来保证线程安全 通过前两种方式获得的线程安全的HashMap在读写数据的时候会对整个容器上锁，而ConcurrentHashMap并不需要对整个容器上锁，它只需要锁住要修改的部分就行了。 CHM引入了分割，并提供了HashTable支持的所有的功能。在CHM中，支持多线程对Map做读操作，并且不需要任何的blocking。这得益于CHM将Map分割成了不同的部分，在执行更新操作时只锁住一部分。根据默认的并发级别(concurrency level)，Map被分割成16个部分，并且由不同的锁控制。这意味着，同时最多可以有16个写线程操作Map。试想一下，由只能一个线程进入变成同时可由16个写线程同时进入(读线程几乎不受限制)，性能的提升是显而易见的。但由于一些更新操作，如put(),remove(),putAll(),clear()只锁住操作的部分，所以在检索操作不能保证返回的是最新的结果。 另一个重要点是在迭代遍历CHM时，keySet返回的iterator是弱一致和fail-safe的，可能不会返回某些最近的改变，并且在遍历过程中，如果已经遍历的数组上的内容变化了，不会抛出ConcurrentModificationExceptoin的异常。 CHM默认的并发级别是16，但可以在创建CHM时通过构造函数改变。毫无疑问，并发级别代表着并发执行更新操作的数目，所以如果只有很少的线程会更新Map，那么建议设置一个低的并发级别。另外，CHM还使用了ReentrantLock来对segments加锁。 什么时候使用ConcurrentHashMapCHM适用于读者数量超过写者时，当写者数量大于等于读者时，CHM的性能是低于Hashtable和synchronized Map的。这是因为当锁住了整个Map时，读操作要等待对同一部分执行写操作的线程结束。CHM适用于做cache,在程序启动时初始化，之后可以被多个请求线程访问。正如Javadoc说明的那样，CHM是HashTable一个很好的替代，但要记住，CHM的比HashTable的同步性稍弱。 CHM允许并发的读和线程安全的更新操作 在执行写操作时，CHM只锁住部分的Map 并发的更新是通过内部根据并发级别将Map分割成小部分实现的 高的并发级别会造成时间和空间的浪费，低的并发级别在写线程多时会引起线程间的竞争 CHM的所有操作都是线程安全 CHM返回的迭代器是弱一致性，fail-safe并且不会抛出ConcurrentModificationException异常 CHM不允许null的键值 可以使用CHM代替HashTable，但要记住CHM不会锁住整个Map 参考：http://www.importnew.com/21388.html http://www.importnew.com/21388.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Web——Servlet开发入门]]></title>
    <url>%2F2017%2F08%2Fjava-web-servlet-development-portal.html</url>
    <content type="text"><![CDATA[Servlet是什么？Servlet，Server Applet,是一个Java编写的小程序，可以在Web服务器上运行（如Tomcat），它通过HTTP协议接受和响应客户端请求。客户端发送请求给Web服务器，服务器将请求信息发送给Servlet，Servlet生成响应，传递给服务器，服务器再响应至客户端。Servlet在Java中是一个接口，用于定义规则和拓展功能。 Servlet生命周期 初始化init客户端发送请求访问servlet程序，Tomcat会在内存中查找是否存在该servlet的对象，如不存在，则调用init方法创建servlet对象，如已经存在，则直接使用。 运行serviceTomcat为请求创建ServletRequest对象和ServletResponse对象，每执行一次service方法都从ServletRequest中获取请求参数，通过ServletResponse发送响应给客户端。 销毁destory当服务器关闭或者Web项目移除时，servlet会关闭，Tomat调用servlet的destory方法进行销毁，释放资源。 HttpServletServlet是一个无协议的接口，定义了服务器和java程序之间规则。HttpServlet是它的一个子类，具有Http协议，它的service方法会对请求判断是执行doGet方法还是doPost方法，因此书写Servlet程序需要继承HttpServlet类，并复写doGet方法和doPost方法。 1234567891011121314151617package $&#123;enclosing_package&#125;;import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;public class $&#123;primary\_type\_name&#125; extends HttpServlet &#123; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; &#125; protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; &#125;&#125; web.xml配置Servlet通过web.xml配置，让url路径访问对应的servlet程序 123456789101112131415&lt;servlet&gt; &lt;servlet-name&gt;servlet名称&lt;/servlet-name&gt; &lt;servlet-class&gt;servlet程序的绝对路径&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;servlet名称，与上面保持一致建立映射&lt;/servlet-name&gt; &lt;url-pattern&gt;访问路径，格式为"/具体内容"，表示的URL为localhost:8080/web项目名/具体内容&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 相对路径和绝对路径 客户端的路径，比如利用页面访问，一般使用绝对路径，后面加上项目名称/:指的的是绝对路径，表示http：//ip:port不带/表示相对路径，表示当前文件所在的父目录的路径 服务端路径，比如web.xml/,绝对路径，代表http：/ip:port/项目名称 ServletConfig在开发中，servlet创建有时需要给定一些初始化参数，web服务器会把这些初始化信息封装到ServletConfig中，创建servlet时调用的init方法时，会将初始化信息传递给servlet，因此可以看作web程序的局部配置。 初始化信息在web.xml中的servlet标签中，以key和value的形式设置。获取初始化信息通过API getInitParameter(String name) 根据key获取value和 getInitParameterNames() 返回所有key的枚举 ServletContext每一个web项目都是运行在java虚拟机中的一个web程序，它拥有一个与之对应的上下文对象，即ServletContext，它可以提供web程序中所有servlet共享的信息，即web程序的全局配置。 ServletContext的设置是在web.xml的根目录中,想要获取配置信息需要先获取ServletContext对象。获取ServeltContext可以通过ServletConfig中的getServetContext()获取，或者通过继承的GenericServlet中的getServletContext()方法直接获取.然后通过ServetContext中的getInitParameterNames()或getInitParameterNames()获取全局配置信息. ServletContext可以看作web程序的一个公共容器，一个web程序所有servlet都可以获取其中的数据。 增加：setAttribute(String name,Object object) 删除：removeAttribute(String name 获取：getAttribute(String name) ServeltContext读取文件路径 getRealPath(String path) 返回的是绝对路径，工程发布后，在电脑中的的实际路径 getContextPath() 获取项目的相对路径]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>servet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA单例模式的七种写法]]></title>
    <url>%2F2017%2F06%2Fjava-singleton.html</url>
    <content type="text"><![CDATA[JAVA单例模式的七种写法 第一种（懒汉，线程不安全）： 123456789101112public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 懒加载，但是多线程单例会失败。 第二种（懒汉，线程安全）： 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 线程安全，效率低，大多数情况下不需要同步。 第三种（饿汉）： 1234567public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 避免了线程安全问题，但是instance在类加载的时候就实例化了，不能达到懒加载的效果。 第四种（饿汉，变种）： 12345678910public class Singleton &#123; private Singleton instance = null; static &#123; instance = new Singleton(); &#125; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return this.instance; &#125; &#125; 和第三种实际上差不多。类初始化的时候实例了instance 第五种（静态内部类）： 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 这种方式同样利用了classloder的机制来保证初始化instance时只有一个线程，它跟第三种和第四种方式不同的是：第三种和第四种方式是只要Singleton类被装载了，那么instance就会被实例化（没有达到lazy loading效果），而这种方式是Singleton类被装载了，instance不一定被初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显示装载SingletonHolder类，从而实例化instance。 静态内部类和非静态内部类一样，都是在被调用时才会被加载。当只调用外部类的静态变量，静态方法时，是不会让静态内部类的被加载。 不过在加载静态内部类的过程中也会加载外部类。 第六种（枚举）： 12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; 它不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象，jdk1.5以上支持 第七种（双重校验锁）： 1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125; 只锁定singleton==null的情况，一旦实例化后就不再同步。但是在singleton=new Singleton();这一步jvm执行是无序的，可能先设置了singleton为非空，再构造了Singleton，若此时其它线程进入了判断，singleton！=null，返回的singleton并不是Singleton的实例。 在这里用到了volatile关键字，一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 禁止进行指令重排序]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[盘点我的2016电子产品消费]]></title>
    <url>%2F2016%2F12%2F2016.html</url>
    <content type="text"><![CDATA[2016年12月31，毕业已经一年半。我算是个喜欢捣鼓电子产品的人，大学的时候是个穷学生，囊肿羞涩，很多喜欢的电子产品只能网上看看止渴，现 在工作一年多，也算是有了一点点经济收入，可以尝试一下自己钟情已久的一些小玩意，在2016年的最后一年，盘点一下这一年剁手的电子产品。 2016年剁手产品1.NOKIA N1 诺 基亚（Nokia）N1平板, 7.9英寸 火山灰,年初3月购于京东。我们这个年龄的年轻人们都是经历了诺基亚的时代过来的，多多少少对诺基亚都有一份情怀，我也是如此。可惜这个平板只是贴了诺基 亚的牌而已，实际上由富士康生产，不过其素质还算可圈可点。2K的分辨率、7.9英寸全金属机身，虽然不支持扩展存储，但是机身存储32G也算够用，主要 是价格足够吸引人。算起来它的短板也不少，配置渣渣，系统万年5.0，4：3的比例看起视频来黑边实在碍眼，好在颜值不低，而且我的需求也只是用来看漫画 和PDF,这两点可以说是相当令我满意。2K的分辨率相当细腻，配合Perfect Viewer看看漫画，看看PDF文档，当然还有安卓神器EhViewer，玩游戏时候查查攻略，不过今年加班一直比较多，导致N1吃灰的时间也不少。 2.AOC P2491VWHE AOC P2491VWHE/BW， 23.6英寸1.6mm PLS显示器，4月购于京东。这个是今年我感觉买的非常值的东西了，终于不用再忍受笔记本那渣渣的分辨率和小显示屏了，看电影刷网页，爽的飞起，真是早就 该撸一个爽爽了。而其本身性价比和质量、显示效果只能说一般般，没有内置音箱，也没有音频输出接口，不过对于习惯了笔记本后改用这个所带来的感官体验还是 很棒的，自从入了这个，下电影基本都是1080P了，这样才够爽。 3.ATH-IM50 铁 三角 ATH-IM50 双动圈入耳耳机，京东618活动入，用了减100券328入手。在此之前我一直很排斥入耳式耳机，我耳腔比较小，入耳式的耳机感觉带着很不舒服，但是入了 这个之后改变了我对入耳式耳机的看法，带着真是很舒服，耳挂式的设计也很好，虽然只是入门级的产品，双动圈的体验已经让我这个木耳党感觉非常惊艳了，入了 这个之后，手里的铁三角SQ505、T400、潜39_、_仿314P的蜂鸟我基本都没再听过。由于木耳，就不对玄学方面发表过多评价了，反正效果真的很棒，甚至后悔当初为啥没选IM70，当初感觉稍贵而且红色实在太骚难以驾驭，现在觉得要是入了IM70只会更爽啊，只能暗示自己反正我是木耳，没啥区别喽！ 4.建兴T9 128G SSD 建 兴(LITEON) 睿速系列 T9 128G SATA3 固态硬盘，主控Marvell 88SS9187， eMLC颗粒。8月购于京东，自己装了笔记本之后又帮同事买了一块换上。其实想上SSD很久了，但是懒癌发作一直懒得动，而且笔记本性能又太渣，感觉上了 效果也是打折扣，但是越来越受不了火狐那龟速的启动速度，potplay看了电影还得等了2S，感觉非常不爽，于是一怒果断上了SSD,建兴这个算是最便 宜的一款MLC SSD了，总体来说还是令人满意的，虽然笔记本还是渣渣，但是各种软件启动速度还是大大得到了提升，无奈笔记本性能有限，没法弄几个3A大作读条爽一把 了。 5.SONY PS4 SONY PlayStation 4 港版 500G 1206型，9月在淘宝 S1版主的店里买的，索尼大法好！总算是充值了信仰，圆了一个梦。心动PS4很久了，却一直没下定决心买。最初是有意PSV的，大学PSV玩的闪轨，为了 玩到下一部轨迹，考虑过PSV,但是暴死V实在是没太多游戏可玩，而且感觉Falcom也该上PS4，但是轨迹的消息一直迟迟未出，东京迷城和伊苏8相继 发售，而且都会登录PS4，所以一直在观望状态。一直到9月，感觉那段时间心力憔悴，想找个发泄口，大半年都没再看动画了，找了几部也提不起兴趣，于是决 定入台PS4充值信仰，当时Slim已经泄漏,Pro，不对那个时候代号还是NEO的各种谣言满天飞，slim的外观感觉没有老版漂亮，于是直接入了港版 PS4，第一个入手的游戏是神秘海域4，也是淘宝买的实体版，当时真的是被画面和演出给震惊到了，游戏全程真的非常享受，如同欣赏一部波澜起伏的故事，而 这个故事是由我自己亲自操控的，真是带来了不一样体验，我是一个动作游戏苦手，也就只能玩玩鬼泣和暗黑血统这些动作游戏，大多数都是在玩RPG和一些 ARPG，神海4让我成为了顽皮狗粉，于是后来陆陆续续补了顽皮购的美国末日、神海1、2、3,并且接下来期待着神海4DLC和美末2。不过PSN游戏价 格和Steam比起来真是贵了好多，虽然实体版可以二手回血，但是往往数字版有打折活动，而且收藏癖发作实在不忍心卖二手，所以买游戏只能赶上打折或者非 常心动才舍得入手。前面也说了笔记本太渣，大学毕业之后基本就没玩过啥单机游戏了，入了PS4之后总算重拾了起来，下班之后玩会游戏放松一下，刷刷奖杯乐 此不疲，请让我高呼一声：索尼大法好啊！而明年闪轨三也要发售，PS4独占，只能对V公主说一声可惜了 6.The Witcher 3 Game of the Year Edition (PS4) 巫 师三—狂猎年度版。之所以把这个单独拿出来说，是因为这个比较特殊，我的第一次海淘，11月底购于英国亚马逊，算上邮费一共£19.87，前前后后花了半 个月多才到手。大学的时候忍受着20多帧玩了巫师二，久久难以忘怀，后来狂猎发售，好评如潮，但是没有设备可以体验，心理痒痒不已，入了PS4后终于可以 弥补一下。正好黑五那段时间英亚巫师三一直打折，而且自带繁体中文，价格也够便宜，比港服当时的活动6折还要便宜几十块，正好前段时间信用卡也申请成功， 于是果断下单。不得不说时间真的挺长的，从荷兰的阿姆斯特丹发货到我这大郑姆斯特丹，真是等了好久。上手之后感觉波兰人真是厚道，这游戏确实可以称得上是 波兰人的骄傲，不多说了，来局昆特盘吧！ 7.希捷 1T笔记本硬盘 希 捷(SEAGATE)1T 5400转128M SATA3 笔记本硬盘(ST1000LM035)，这个没啥好说了，给PS4换的。一直在观望想买个1T或者2T的硬盘给PS4换上，结果硬盘价格涨的好厉害，碰到 双12京东总算舍得打折，于是入了块1T的。不得不说现在这PS4游戏真是蛮大的，即使是实体版也是要把游戏拷贝到机身里，光盘只是一个验证的作用，神海 四 58G、GTA5 65G 、巫师三 48G……想想几年前玩的游戏，变化真是大。 2016年想体验的产品——VR、AR VR 在16年炒得真是非常火爆，低端入门的各种盒子、眼镜，中高端的索尼PlayStation VR、HTC Vive、Oculus Rift，似乎以前科幻电影中的技术一下子走到了我们生活中，至于VR的前景和各大厂商产品的优劣在这里不在多说，我个人而言还是很希望这个技术能够良好 的发展下去，最终能点出脑后插管的黑科技。虽然技术的不成熟、游戏的单调与缺乏，即使是评价最好的PS VR也渐渐不在吸引人们的眼球，我还是对VR充满着信心，同理微软的AR HoloLens也是如此，希望17年能有机会亲自体验一下。 2017年期待产品——Nintendo Switch 不 得不说，任天堂的 Switch真是让人感觉到惊艳，掌机便携但是性能、显示效果有限，主机体验效果好但是便携性打了大大的折扣，而Switch让二者合二为一的设计可以说 是玩家一直以来的一个梦想。虽然现在已经曝光了很多信息，至于到底其实际体验效果会如何，还是期待明年3月的发布会，希望任天堂能带来惊喜，再创一个传 奇。]]></content>
      <categories>
        <category>电子产品</category>
      </categories>
      <tags>
        <tag>电子产品</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java类加载以及对象创建过程]]></title>
    <url>%2F2016%2F12%2Fjava-classload.html</url>
    <content type="text"><![CDATA[对于Java类加载以及对象创建的过程，首先看一下JVM虚拟机运行时内存中的数据分布 分布 私有： 栈内存Stack：栈结果，先进后出，方法进栈执行，执行完毕出栈，方法中的局部变量、操作数栈、动态链接、方法返回值都保存在此处。 本地方法区：加载本地非java语言代码的入口 程序计数器：由于多线程CPU在各线程之间来回切换，因此程序计数器来标记当前线程执行到的代码的位置 共享： 堆内存：存放对象实例，占据内存最大的一块区域，有独特的GC回收机制回收没有引用关联的对象，可分为年轻代和年老代 方法区：存储加载的类信息、常量区、静态变量、JIT（即时编译器）处理后的数据等 类加载及对象创建过程： 启动JVM，开始分配内存空间 将编译后的.class文件加载到方法区，非静态内容加载到非静态的区域，静态内容加载到静态区，静态方法调用的时候才会加载 给静态成员变量默认初始化，默认初始化完成之后开始显示初始化 执行静态代码块，先执行父类的静态代码块再执行子类的静态代码块 执行main方法的时候，栈内存开辟空间，压栈–进栈 main方法的栈区分配了一个变量P 在堆内存中开辟一个对象空间，首先会去方法区查找类是否加载 类已经加载，开始对象创建，在对象空间对非静态成员变量默认初始化 加载对应构造函数，执行隐式三步 隐式的super(); 显示初始化(给所有的非静态的成员变量) 执行构造代码块 执行构造函数中的代码]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>jvm</tag>
        <tag>类加载</tag>
      </tags>
  </entry>
</search>
